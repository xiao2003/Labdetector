# pcside/knowledge_base/rag_engine.py
import os
import time
import logging
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from pcside.core.logger import console_info, console_error

os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
os.environ["TRANSFORMERS_VERBOSITY"] = "error" # å±è”½ BertModel UNEXPECTED æç¤º
logging.getLogger("httpx").setLevel(logging.WARNING) # å±è”½ HTTP Request æç¤º
logging.getLogger("sentence_transformers").setLevel(logging.WARNING)
logging.getLogger("huggingface_hub").setLevel(logging.ERROR)

from langchain_community.document_loaders import TextLoader

class RAGEngine:
    def __init__(self):
        # 1. è·¯å¾„è‡ªåŠ¨è§„åˆ’
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.db_path = os.path.join(self.base_dir, "faiss_index")
        # å­˜æ”¾æ‰€æœ‰ txt æ–‡æœ¬çš„ç›®å½•
        self.docs_dir = os.path.join(self.base_dir, "docs")
        os.makedirs(self.docs_dir, exist_ok=True)

        from pcside.core.logger import console_info  # ç¡®ä¿é¡¶éƒ¨æˆ–è¿™é‡Œå¯¼å…¥äº† console_info
        console_info(" æ­£åœ¨åŠ è½½ RAG æœ¬åœ°è¯­ä¹‰å‘é‡æ¨¡å‹ï¼Œåˆæ¬¡å¯åŠ¨é€šå¸¸éœ€è¦å‡ åç§’é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")

        # 2. åˆå§‹åŒ– Embedding æ¨¡å‹ (ä¸­æ–‡æ¨è text2vec)
        try:
            self.embeddings = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")
            self.vector_db = None
            self._init_db()
        except Exception as e:
            console_error(f"RAG å¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹ä¾èµ–: {e}")

    def _init_db(self):
        """åŠ è½½æˆ–æ–°å»º FAISS å‘é‡æ•°æ®åº“"""
        if os.path.exists(self.db_path):
            self.vector_db = FAISS.load_local(self.db_path, self.embeddings, allow_dangerous_deserialization=True)
            console_info(" å·²æˆåŠŸåŠ è½½æœ¬åœ°å®éªŒå®¤çŸ¥è¯†åº“")
        else:
            # åˆå§‹åŒ–ä¸€ä¸ªç©ºåº“
            self.vector_db = FAISS.from_texts(["å®éªŒå®¤çŸ¥è¯†åº“åˆå§‹åŒ–å®Œæˆ\n"], self.embeddings)
            self.vector_db.save_local(self.db_path)
            console_info(" RAG: å°šæœªå‘ç°çŸ¥è¯†åº“ï¼Œå·²åˆ›å»ºå…¨æ–°å‘é‡æ•°æ®åº“\n")

    def save_and_ingest_note(self, text_content: str) -> bool:
        """
        æ ¸å¿ƒåŠŸèƒ½ï¼šå°†è¯­éŸ³è¯†åˆ«çš„æ–‡æœ¬å­˜ä¸º TXTï¼Œå¹¶ç«‹å³å½•å…¥å¤§æ¨¡å‹è®°å¿†
        """
        if not text_content or not text_content.strip():
            return False

        # 1. ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„ txt æ–‡ä»¶
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        filename = f"VoiceNote_{timestamp}.txt"
        filepath = os.path.join(self.docs_dir, filename)

        try:
            # 2. ä¿å­˜ä¸ºç‰©ç†æ–‡ä»¶ï¼Œæ–¹ä¾¿äººç±»åç»­ç›´æ¥æŸ¥é˜…
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(text_content)

            # 3. ç«‹å³å‘é‡åŒ–å¹¶å­˜å…¥ FAISS
            loader = TextLoader(filepath, encoding='utf-8')
            docs = loader.load()
            # è¯­éŸ³ç‰‡æ®µé€šå¸¸è¾ƒçŸ­ï¼Œé€‚å½“ç¼©å° chunk
            splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)
            splits = splitter.split_documents(docs)

            self.vector_db.add_documents(splits)
            self.vector_db.save_local(self.db_path)

            console_info(f"ğŸ“ è¯­éŸ³è®°å¿†å·²å­˜æ¡£è‡³ [{filename}] å¹¶å®Œæˆå‘é‡å­¦ä¹ ï¼")
            return True
        except Exception as e:
            console_error(f"è¯­éŸ³è®°å¿†å…¥åº“å¤±è´¥: {e}")
            return False

    def retrieve_context(self, query: str, top_k: int = 3) -> str:
        """æ£€ç´¢ç›¸å…³çŸ¥è¯†"""
        if not self.vector_db: return ""
        docs = self.vector_db.similarity_search(query, k=top_k)
        return "\n---\n".join([doc.page_content for doc in docs])


# é‡‡ç”¨å•ä¾‹æ¨¡å¼å¯¼å‡ºï¼Œç¡®ä¿å…¨å±€åªæœ‰ä¸€ä¸ªæ•°æ®åº“å®ä¾‹
rag_engine = RAGEngine()