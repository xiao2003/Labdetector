#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
rag_test.py - æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) ç‹¬ç«‹æµ‹è¯•å¼•æ“
"""
import os
import time
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.prompts import PromptTemplate

# é…ç½®è·¯å¾„
KB_DIR = "./knowledge_base"
# æˆ‘ä»¬æ¨èä½¿ç”¨ Qwen æ¥åšçº¯æ–‡æœ¬æ¨ç†ï¼Œé€Ÿåº¦æå¿«ä¸”ä¸­æ–‡é€»è¾‘å¥½
# ç¡®ä¿ä½ å·²ç»åœ¨ç»ˆç«¯è¿è¡Œè¿‡: ollama run qwen2.5:7b (æˆ–æ›´é«˜ç‰ˆæœ¬)
OLLAMA_MODEL = "qwen2.5:7b"


def build_vector_db():
    print("â³ [1/3] æ­£åœ¨åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶...")
    if not os.path.exists(KB_DIR) or not os.listdir(KB_DIR):
        print(f"âŒ é”™è¯¯ï¼šè¯·åœ¨ {KB_DIR} æ–‡ä»¶å¤¹ä¸‹æ”¾å…¥è‡³å°‘ä¸€ä¸ª .txt æ–‡ä»¶ï¼")
        return None

    # åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰çš„ txt æ–‡ä»¶
    loader = DirectoryLoader(KB_DIR, glob="**/*.txt", loader_cls=TextLoader, loader_kwargs={'encoding': 'utf-8'})
    docs = loader.load()
    print(f"âœ… æˆåŠŸåŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£ã€‚")

    print("â³ [2/3] æ­£åœ¨å¯¹æ–‡æœ¬è¿›è¡Œè¯­ä¹‰åˆ‡å—...")
    # åˆ‡å—ç­–ç•¥ï¼šæ¯å— 500 å­—ï¼Œä¿ç•™ 50 å­—çš„é‡å é˜²æ–­å¥
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(docs)
    print(f"âœ… æˆåŠŸå°†æ–‡æœ¬åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªè¯­ä¹‰å—ã€‚")

    print("â³ [3/3] æ­£åœ¨ä¸‹è½½å¹¶åŠ è½½ BGE å‘é‡åŒ–æ¨¡å‹ (é¦–æ¬¡è¿è¡Œéœ€è”ç½‘ä¸‹è½½ï¼Œçº¦1.2GB)...")
    # ä½¿ç”¨å›½äº§æœ€å¼ºå¼€æº embedding æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-large-zh-v1.5")

    print("â³ æ­£åœ¨æ„å»º FAISS æœ¬åœ°å‘é‡æ•°æ®åº“...")
    vector_db = FAISS.from_documents(chunks, embeddings)
    print("âœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼\n")
    return vector_db


def main():
    print("=" * 60)
    print("ğŸ§ª å¾®çº³æµä½“å®éªŒå®¤ - æ™ºèƒ½çŸ¥è¯†é—®ç­”ç³»ç»Ÿ (RAG æµ‹è¯•ç‰ˆ)")
    print("=" * 60)

    vector_db = build_vector_db()
    if vector_db is None: return

    # åˆå§‹åŒ– Ollama æ–‡æœ¬å¤§æ¨¡å‹
    print(f"ğŸ”Œ æ­£åœ¨è¿æ¥æœ¬åœ° Ollama æ¨¡å‹: {OLLAMA_MODEL}...")
    llm = Ollama(model=OLLAMA_MODEL)

    # ç²¾å¿ƒè®¾è®¡çš„ä¸“ä¸šç³»ç»Ÿæç¤ºè¯
    prompt_template = PromptTemplate(
        input_variables=["context", "question"],
        template="""ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„å¾®çº³æµä½“å®éªŒå®¤å®‰å…¨ä¸æŠ€æœ¯ä¸“å®¶ã€‚
è¯·ä¸¥æ ¼åŸºäºä»¥ä¸‹ã€å‚è€ƒçŸ¥è¯†ã€‘æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœå‚è€ƒçŸ¥è¯†ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·ç›´æ¥å›ç­”â€œçŸ¥è¯†åº“ä¸­æš‚æœªæ”¶å½•è¯¥ä¿¡æ¯ï¼Œä¸ºäº†å®éªŒå®¤å®‰å…¨ï¼Œè¯·æŸ¥é˜…å®˜æ–¹è¯´æ˜ä¹¦â€ï¼Œç»ä¸å…è®¸å‡­ç©ºæé€ ï¼ˆå¹»è§‰ï¼‰ã€‚

ã€å‚è€ƒçŸ¥è¯†ã€‘:
{context}

ã€ç”¨æˆ·é—®é¢˜ã€‘: {question}

è¯·ç»™å‡ºä¸“ä¸šã€æ¸…æ™°ã€ç›´æ¥çš„å›ç­”ï¼š"""
    )

    print("\nğŸ‰ ç³»ç»Ÿå·²å°±ç»ªï¼(è¾“å…¥ 'q' æˆ– 'quit' é€€å‡º)")

    while True:
        question = input("\nğŸ‘¤ è¯·æé—®: ").strip()
        if question.lower() in ['q', 'quit', 'exit']:
            break
        if not question:
            continue

        start_time = time.time()

        # 1. å‘é‡æ£€ç´¢ï¼šå¯»æ‰¾æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æœ¬å—
        docs = vector_db.similarity_search(question, k=3)
        context = "\n".join([doc.page_content for doc in docs])

        # 2. ç»„è£… Prompt
        final_prompt = prompt_template.format(context=context, question=question)

        # 3. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”
        print("ğŸ¤– ä¸“å®¶æ€è€ƒä¸­...")
        response = llm.invoke(final_prompt)

        end_time = time.time()
        print(f"\nğŸ’¡ ã€ä¸“å®¶è§£ç­”ã€‘ ({round(end_time - start_time, 2)}ç§’):")
        print(response)


if __name__ == "__main__":
    main()